---
layout: post
title: Python 웹크롤링
summary: 열심히 기록하자!
author: Lee Chang Ho
date: '2020-11-26 10:49:00'
category: 프로젝트
published: true
image:  파이썬.png
tags:   파이썬, Python, Crawling
---

### 노트북 새로 산 기념으로 시작하는 나의 프로젝트....  
### 블로그 쓰는 체질이 아니라 그런지 뭐라 적어야 할지 모르겠다...... 그래도 시작한김에 해야지!

### 첫번째로 Python을 이용하여 데이터를 수집할 것이다. 대상은 네이버 뉴스로 정하였다.  
### 현재 만든 버전은 완벽하진 않지만 기본적으로 데이터 수집은 가능하다. 추후에는 배치 스케줄을 걸어 1일 단위로 수집을 할 계획이다.  
### 파이썬의 경우 필자도 처음 써보는 것이라 구글링을 통해 소스를 만들었다.

#### 준비물은 아래와 같다.
+ 1. Maria DB
+ 2. webDriver(chromeDriver.exe)
+ 3. Python

#### 첫번째로 파이썬으로 수집을 할 수 있는 방법을 찾아보자
##### 구글에 "파이썬 웹크롤링"이라는 키워드로 검색을 해보면 몇가지의 방법이 나온다.  

##### 그 중 나는 webDriver를 이용한 방식을 택하였다.
##### 먼저 크롬 webDriver를 다운로드 받고 원하는 경로에 넣어둔다. 필자의 경우 프로젝트 안에 driver라는 폴더를 만들어 그 안에 넣어놨다.

![이미지]({{ site.url }}/images/webDriver위치.png)

##### 그 뒤에 아래와 같이 Python 코드를 작성하였다.
```python
import time
import pymysql.cursors
import multiprocessing
from selenium import webdriver
from bs4 import BeautifulSoup

def init(pCode):
    driver = webdriver.Chrome('driver/chromedriver.exe')
    time.sleep(2)

    crawling(driver, pCode)

def crawling(pDriver, pCode):

    # mysql ########################## 
    conn = pymysql.connect( host='localhost', user='????', password='????', db='zzangho', charset='utf8' )
    curs = conn.cursor()

    i = 1
    while True:
        print("pagenum :: " + str(i))
        pDriver.get("https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=" + pCode + "#&date=%2000:00:00&page=" + str(i))

        html = pDriver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        #링크
        hrefs = soup.select(
            '#section_body > ul > li > dl > dt:nth-child(1) > a'
        )

        #제목
        titles = soup.select(
            '#section_body > ul > li > dl > dt:nth-child(2) > a'
        )

        #신문사
        companies = soup.select(
            '#section_body > ul > li > dl > dd > span.writing'
        )

        if len(hrefs) <= 0: 
            break
        

        contents = []
        dates = []
        ampms = []
        times = []

        ## 상세페이지 수집(본문내용, 작성일)
        for href in hrefs:
            sub_url = 'https://news.naver.com' + href.attrs['href']

            pDriver.get(sub_url)

            sub_html = pDriver.page_source
            sub_soup = BeautifulSoup(sub_html, 'html.parser')
            content = sub_soup.select_one('div#articleBodyContents').get_text()
            fullDate = sub_soup.select_one('span.t11').get_text()
            date = fullDate.split(" ")[0]
            ampm = fullDate.split(" ")[1]
            time = fullDate.split(" ")[2]

            contents.append(content)
            dates.append(date[:-1])
            ampms.append(ampm)
            times.append(time)

        for item in zip(hrefs, titles, companies, contents, dates, ampms, times):

            sSql = "select * from tb_news where CATEGORY=%s and TITLE=%s and CONTENTS=%s"
            curs.execute(sSql, (pCode, item[1].text, item[3]))
            rows = curs.fetchall()

            if len(rows) == 0:
                iSql = "insert into tb_news(CATEGORY, TITLE, CONTENTS, WRITER, DATE, AMPM, TIME, COMPANY, URL, CRAWLER_DT, UDT_DT) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),NOW())"
                curs.execute(iSql, (pCode ,item[1].text , item[3], '', item[4], item[5], item[6], item[2].text, 'https://news.naver.com' + item[0].attrs['href']))

        conn.commit()

        # 페이지 증가
        i += 1
        
    close(pDriver)
    conn.close()

def close(pDriver):
    pDriver.quit()
    
news_code = ['100','101','102','103','104','105']

if __name__ == '__main__':
    pool = multiprocessing.Pool(processes=2)
    pool.map(init, news_code)
    pool.close()
    pool.join()
```

##### 이 소스를 간략히 설명하자면 멀티 프로세싱으로 2개의 프로세서가 각각 돌면서 news_code 배열의 코드값을 URL 파라메터에 넘겨주어 해당 페이지의 게시글들을 긁어온 뒤 내가 사용할 필드(링크, 제목, 본문, 날짜, 신문사)를 파싱하여 MariaDB에 넣는 소스이다. 이 파일을 실행하면 50번 라인에 연동 한 MariaDB에 데이터가 들어간다. ( 아래 이미지 참고 )

![이미지]({{ site.url }}/images/크롤링데이터.png)

##### 웹 크롤러 소스는 현재 단계에서는 중요한게 아니라 여기까지만 작업하고 다음에 좀 더 세밀하게 들어가보도록 하자
