---
layout: post
title: 자연어 처리
summary: 열심히 기록하자!
author: Lee Chang Ho
date: '2021-01-26 08:52:00'
category: 프로젝트
published: true
image:  NLP.png
tags:   자연어 NLP
---

#### 회사에서 흥미로운 책이 있길래 Study겸 정리해 본다.
###### 본 글은 "예제로 배우는 자연어 처리 기초" 를 참고하여 작성한 내용이다.
---
#### NLP의 역사
---
###### NLP는 다른 분야와 겹치는 영역이며 인공 지능, 언어학, 형식 언어(formal language), 컴파일러와 같은 분야에서 비록됐다. 컴퓨팅 기술이 발전하고 데이터 가용성이 향상되면서 자연어 처리 방식이 변경됐다. 이전에는 전통적인 규칙 기반 시스템을 계산에 사용했다. 오늘날 자연어에 대한 연산은 머신러닝 및 딥러닝 기술을 사용해 수행한다.  

###### 머신러닝 기반 NLP에 대한 주요 작업은 1980년대에 시작됐다. 1980년대에 인공 지능, 언어학, 형식 언어, 연산과 같은 다양한 분야의 개발이 이뤄지면서 NLP라는 주제가 등장했다.  

---
#### 텍스트 분석과 NLP
---
###### 주어진 텍스트 데이터에서 유용한 인사이트를 추출하는 기술을 텍스트 분석이라고 한다. 반면에 NLP는 텍스트 데이터에만 국한되지 않는다. 음성(Voice) 인식과 분석도 NLP의 영역에 속한다. NLP는 크게 자연어 이해(NLU)와 자연어 생성(NLG)이라는 두 가지 유형으로 분류할 수 있다. 이 용어를 설명하자면 다음과 같다.  
- NLU(Natural Language Understanding) : NLU는 연산 능력이 있는 무생물이 말하는 언어를 이해할 수 있는 프로세스를 말한다.
- NLG(Natural Language Generation) : NLG는 연산 능력을 가진 무생물 객체가 인간이 이해할 수 있는 언어로 생각을 표현할 수 있는 프로세스를 의미한다.

###### 예를 들면
- 사람이 기계와 대화할 때 기계는 사람이 하는 말을 이해하기 위해 NLU 프로세스의 도움을 받는다
- 기계는 NLG 프로세스를 사용해 적절한 응답을 생성하고 이를 사람과 공유해 사람이 이해할 수 있도록 한다.

#### 예제 1: 기본적인 텍스트 분석
1. 주피터 노트북을 연다.
2. 새 셀을 삽입한다. 'The quick brown fox jumps over the lazy dog'를 sentence변수에 할당한다.  
![이미지]({{ site.url }}/images/예제1-1.png)
3. 다음 코드를 사용해 quick이라는 단어가 해당 테스트에 속하는지 확인하자
![이미지]({{ site.url }}/images/예제1-2.png)
이 코드는 True를 반환한다.  
4. 다음 코드를 사용해 단어 fox의 위치값(index)을 찾아보자
![이미지]({{ site.url }}/images/예제1-3.png)
이 코드는 16을 반환한다.  
5. lazy라는 단어가 공백을 기준으로 몇번째에 있는지를 찾으려면 다음 코드를 사용한다.
![이미지]({{ site.url }}/images/예제1-4.png)
이 코드는 7을 반환한다. split()은 공백을 기준으로 자르는 역할을 한다.  
6. 주어진 텍스트의 세 번째 단어를 출력하려면 다음 코드를 사용한다.
![이미지]({{ site.url }}/images/예제1-5.png)
'brown'이라는 단어를 반환한다.  
7. 주어진 문장의 세 번째 단어를 역순으로 출력하려면 다음 코드를 사용한다.
![이미지]({{ site.url }}/images/예제1-6.png)
'brown'의 거꾸로 인 'nworb'를 반환한다.  

###### 이외에도 몇 가지의 예제가 더 있지만 너무 기본적인 것들이라 스킵하도록 한다.  

---
#### NLP의 다양한 단계
---
- 토큰화  
---
###### 토큰화(Tokenization)는 문장을 구성 단어로 나누는 절차를 말한다. 문장을 일정한 규칙에 따라 자른다라고 보면 될 것 같다. 예제에서는 한 번에 하나씩 토큰을 추출하는 유니그램을 사용할 예정이다. 하지만 한 번에 두 개나 세 개의 토큰을 추출할 수도 있다. (여러개의 단어가 이어져야 의미가 있는 단어가 있기 때문)  
###### 한 번에 두 개의 토큰을 추출하면 바이그램(bigram)이라고 한다. 세 개의 토큰인 경우에는 트라이그램(trigram)이라고 한다. 요구 사항에 따라 n-그램을 추출할 수 있다.(여기서 'n'은 자연수를 의미한다. n개)  

#### 예제 2: 단순 문장의 토큰화
###### 이 예제에서는 NLTK 라이브러리를 사용해 주어진 문장의 단어를 토큰화해본다.  
1. 주피터 노트북(jupyter notebook)을 연다.
2. 새 셀을 삽입하고 다음 코드를 추가해 필요한 라이브러리를 불러온다.
![이미지]({{ site.url }}/images/예제2-1.png)
3. word_tokenize() 함수는 문장을 단어/토큰으로 나누는 데 사용한다. word_tokenize()함수에 input 데이터로 문장을 넣어 작업을 수행한다. 이로써 얻는 결과는 words 변수에 저장할 리스트다.
![이미지]({{ site.url }}/images/예제2-2.png)
4. 생성한 토큰 리스트를 확인하려면 print()함수를 이용하면 된다.
![이미지]({{ site.url }}/images/예제2-3.png)  

- PoS 태깅
---
###### PoS(Parts-of-Speech)는 품사를 의미한다.  내가 이해하는 PoS태깅의 개념은 사람의 언어를 프로그래밍적으로 분석할 수 있도록 태그정보 태깅하여 제공하는 것이다.  
###### 'The sky is blue' 라는 문장의 경우 'sky' 명사인지 사람의 경우 문장을 보면 바로 알 수가 있는데 컴퓨터는 'sky'가 명사인지 조사인지 모른다. 그렇기 때문에 자연어 처리를 할 때 PoS Tag값을 셋팅해 주어 분석을 할 수 있도록 해준다.  위의 예시로 든 'The sky is blue'의 경우 PoS 태거를 사용하면 아래와 같이 태그정보가 보일 것이다.    
```python
[('The'), 'DT'), ('sky', 'NN'), ('is', 'VBZ'), ('blue', 'JJ')]
# DT = 한정사
# NN = 명사, 보통 명사, 단수 명사, 불가산 명사
# VBZ = 동사, 현재 시제, 3인칭 단수
# JJ = 형용사 
```  
  
#### 예제 3: PoS 태깅
###### 이 예제에서는 'I am reading NLP Fundamentals'라는 문장의 각 단어에 대한 PoS를 찾는다. 순서는 먼저 토큰을 얻기 위해 토큰화를 진행하고 그 후 PoS 태거를 사용해 각 단어토큰에 대한 PoS를 찾는다.  

1. 주피터 노트북(jupyter notebook)을 연다.
2. 새 셀을 삽입하고 다음 코드를 추가해 필요한 라이브러리를 불러온다.
![이미지]({{ site.url }}/images/예제3-1.png)  
3. 문장에서 토큰을 찾기 위해 먼저 word_tokenize() 함수를 사용한다.
![이미지]({{ site.url }}/images/예제3-2.png)  
4. 각 단어에 대한 PoS를 찾기 위해 nltk 라이브러리의 pos_tag() 함수를 사용한다.
![이미지]({{ site.url }}/images/예제3-3.png)  

- 불용어 제거
---
###### 불용어(stop word)는 문장의 구성을 지원하기 위해 사용하는 일반적인 단어다. 문장의 의미에는 큰 영향을 미치지 않으므로 분석할 때는 제거한다.  
###### 보통 문장에서 뽑히면 안되는 토큰을 불용어 처리하여 토큰 리스트에서 제거한다. 실무에서는 그 외에 성적인 단어나 비하 단어, 욕설 등을 불용어로 처리를 하고 있다.  

#### 예제 4: 불용어 제거  

1. 주피터 노트북(jupyter notebook)을 연다.
2. 새 셀을 삽입하고 다음 코드를 추가해 필요한 라이브러리를 불러온다.
![이미지]({{ site.url }}/images/예제4-1.png)  
nltk.download('stopwords')의 경우 불용어 사전을 다운로드 받는다고 보면 된다.  
3. English에 대해 제공되는 불용어 리스트를 확인하기 위해 words() 함수에 파라미터로 전달한다.
![이미지]({{ site.url }}/images/예제4-2.png)  
4. 코드에서 English에 대해 제공되는 불용어 리스트는 stop_words 변수에 저장된다. 리스트를 확인하기 위해 print()함수를 사용한다.
![이미지]({{ site.url }}/images/예제4-3.png)  
5. 문장에서 불용어를 제거하기 위해 먼저 sentence 변수에 문자열을 할당하고 word_tokenize() 함수를 사용해 단어로 토큰화한다. 
![이미지]({{ site.url }}/images/예제4-4.png)  
6. 불용어를 제거하기 위해 먼저 문장의 각 단어를 순회하면서 불용어인지 확인 한 후, 마지막에 단어들을 조합해 완전한 문장을 만들어야 한다.
![이미지]({{ site.url }}/images/예제4-5.png)  
위 그림에서 볼 수 있듯이 'am', 'is', 'of', 'the', 'most'와 같은 불용어가 필터링되고, 불용어가 제거된 텍스트만 출력으로 생성된다.  

- 텍스트 정규화
---
###### 뭄바이와 봄베이, US와 United States처럼 다르게 발음되고 다른 형태로 표현되지만, 의미적으로는 동일한 단어들이 있다. 기본형으로 변환해야 하는 다른 형식의 단어도 있다. 예를 들어 'does'와 'doing'같은 단어는 기본 형식으로 변환될 때 'do'가 된다. 이렇게 텍스트 정규화(text normalization)는 텍스트의 다른 변형이 표준 형식으로 변환되는 프로세스를 의미한다. 서로 같은 것을 의미할 수 있는 단어가 있으므로 텍스트 정규화를 수행해야 한다. 텍스트 정규화로는 철자 수정, 어간 추출, 표제어 추출과 같이 여러 가지 방법이 있다.   

#### 예제 5: 텍스트 정규화
###### 이 예제에서는 주어진 텍스트를 정규화한다. 기본적으로 replace() 함수를 사용해 선택한 단어를 새 단어로 바꾸고 정규화한 텍스트를 생성한다.  

1. 주피터 노트북(jupyter notebook)을 연다.
2. 새 셀을 삽입하고 다음 코드를 추가해 sentence 변수에 문자열을 할당한다.
![이미지]({{ site.url }}/images/예제5-1.png)  
3. US를 United States로, UK를 United Kingdom으로,  18을 2018로 바꾸어 보자.이를 위해 replace() 함수를 사용하고, 업데이트한 출력을 normalized_sentence변수에 저장한다. 
![이미지]({{ site.url }}/images/예제5-2.png)  
4. 이제 텍스트가 정규화 됐는지 확인해보자
![이미지]({{ site.url }}/images/예제5-3.png)  

- 철자 수정
---
###### 철자 수정 또는 오타 교정이라고 한다. 모든 NLP 프로젝트에서 가장 중요한 작업 중 하나다. 시간이 많이 소요될 수 있지만, 이 과정이 없다면 필요한 정보를 잃어버릴 가능성이 높다. autocorrect 파이썬 라이브러리를 사용해 철자를 수정해보자.  

#### 예제 6: 단어와 문장의 철자 수정
###### 이 예제에서는 파이썬의 autocorrect 라이브러리를 사용해 단어와 문장에서 철자를 수정해 본다.

1. 주피터 노트북(jupyter notebook)을 연다.
2. 새 셀을 삽입하고 다음 코드를 추가해 필요한 라이브러리를 불러온다.
![이미지]({{ site.url }}/images/예제6-1.png)  
3. 단어의 철자를 수정하려면 철자가 잘못된 단어를 spell() 함수에 파라미터로 전달한다.
![이미지]({{ site.url }}/images/예제6-2.png)  
4. 문장의 철자를 수정하려면 먼저 단어를 토큰화해야 한다. 그런 다음에 sentence변수의 각 단어를 반복해 자동 수정하고, 마지막으로 결합한다.
![이미지]({{ site.url }}/images/예제6-3.png)  
5. 이제 토큰을 얻었으므로 sentence변수에 있는 각 토큰을 반복하면서 수정해 새 변수에 할당한다.
![이미지]({{ site.url }}/images/예제6-4.png)  

- 어간 추출
---
###### 영어와 같은 언어에서 단어는 문자에 사용될 때 다양한 형태로 변형된다. 예를 들어 'product'라는 단어는 복수의 형태로 표현될 때는 'products'로, 변형되는 과정을 나타낼 때는 'production'으로 변형될 수 있다. 이 단어들은 같은 의미를 가지고 있기 때문에 기본 단어로 변환해야 한다. 어간 추출은 이 변환 과정을 도와주는 프로세스이다.  
![이미지]({{ site.url }}/images/어간추출.png)
  
#### 예제 7: 어간 추출

1. 주피터 노트북(jupyter notebook)을 연다.
2. 새 셀을 삽입하고 다음 코드를 추가해 필요한 라이브러리를 불러온다.
![이미지]({{ site.url }}/images/예제7-1.png)
3. 이제 다음 단어들을 stem() 함수에 파라미터로 전달한다.
![이미지]({{ site.url }}/images/예제7-2.png) 

- 표제어 추출
---
###### 때로는 어간 추출 과정으로 인해 부적절한 결과가 발생할 수 있다. 예를 들어, 'battling'이라는 단어는 'battl'로 변환이 되며, 이는 의미를 갖지 않는다. 어간 추출에서 이러한 문제를 극복하기 위해 표제어 추출을 사용한다. 이 과정에서 사전을 통해 단어의 기본 형태를 추출해 추가적인 검사를 수행한다. 하지만 이 추가 검사는 프로세스 속도를 떨어드린다.  

#### 예제 8: 표제어 추출을 사용해 기본 단어 추출

1. 주피터 노트북(jupyter notebook)을 연다.
2. 새 셀을 삽입하고 다음 코드를 추가해 필요한 라이브러리를 불러온다.
![이미지]({{ site.url }}/images/예제8-1.png)
3. WordNetLemmatizer 클래스의 객체를 만든다.
![이미지]({{ site.url }}/images/예제8-2.png)
4. WordNetLemmatizer 클래스의 lemmatize() 함수를 사용해 단어를 적절한 형태로 만든다.
![이미지]({{ site.url }}/images/예제8-3.png)

- NER
---
###### 개체명(Named entity)은 일반적으로 사전에 없다. 따라서 별도로 처리해야 한다. 이 과정의 주요 목표는 개체명을 식별하고 이미 정의한 범주에 매핑하는 것이다. 예를 들어 범주에는 사람, 장소 등의 이름이 포함될 수 있다.  

#### 예제 9: 개체명 취급

1. 주피터 노트북(jupyter notebook)을 연다.
2. 새 셀을 삽입하고 다음 코드를 추가해 필요한 라이브러리를 불러온다.
![이미지]({{ site.url }}/images/예제9-1.png)
3. sentence 변수를 선언하고 문자열을 할당한다.
![이미지]({{ site.url }}/images/예제9-2.png)
4. 위 텍스트에서 개체명을 찾으려면 다음 코드를 삽입한다.
![이미지]({{ site.url }}/images/예제9-3.png)
위 그림에서 코드는 개체명 'Packt'와 'Birmingham'을 식별하고 'NNP'와 같이 이미 정의된 범주에 매핑할 수 있음을 알 수 있다.
 
- 단어 중의성 해결
---
###### '사귀는 친구를 보면 그 사람을 알 수 있다'는 유명한 말이 있다. 이와 마찬가지로, 단어의 의미도 문장의 다른 단어와 어떤 연관이 있는지에 따라 다르다. 이는 철자가 동일한 두 개 이상의 단어가 다른 상황에서 서로 다른 의미를 가질 수 있음을 의미한다. 이것은 종종 모호성을 초래한다. 단어 중의성 해결은 단어를 올바른 의미로 매핑하는  과정이다. 분석할 때 다른 개체로 취급될 수 있도록 단어의 의미에 따라 명확하게 표현해야 한다.  

#### 예제 10: 단어 중의성 해결
###### 이 예제에서는 서로 다른 두 문장에 있는 단어 'bank'의 의미를 찾아본다. 

1. 주피터 노트북(jupyter notebook)을 연다.
2. 새 셀을 삽입하고 다음 코드를 추가해 필요한 라이브러리를 불러온다.
![이미지]({{ site.url }}/images/예제10-1.png)
3. 두 개의 변수 sentence1과 sentence2를 선언하고 적절한 문자열로 할당한다. 
![이미지]({{ site.url }}/images/예제10-2.png)
4. 앞의 두 문장에서 'bank'라는 단어의 정확한 의미를 찾기 위해 nltk.wsd 라이브러리에서 제공하는 lesk 알고리즘을 사용하자.  
###### lesk 알고리즘 : 1980년대 후반에 등장한 최초의 지식 기반 WSD 알고리즘. 이 분야에서는 고전이라고 할 수 있다. 중의성을 해소할 어구에 포함된 단어들의 뜻풀이를 나열하고 그 뜻풀이 중에서 겹치는 단어를 최대로 하는 뜻풀이들을 선택하는 것  
![이미지]({{ site.url }}/images/예제10-3.png)
여기서 savings_bank.n.02는 집에서 돈을 안전하게 지키기 위한 컨테이너를 의미한다. 그리고 bank.v.07은 도로 회전의 경사도를 나타낸다.  

- 문장 경계 인식
---
###### 문장 경계 인식(sentence boundary detection)은 한 문장이 끝나는 것과 다른 문장이 시작되는 곳을 인식하는 방법이다. 즉, 쉽게 말하면 긴 텍스트에서 문장을 기준으로 나눈다고 보면 된다. 마침표(.)가 문장의 끝과 다른 문장의 시작을 나타내기 때문에 꽤 쉽다고 생각한다면, 그것은 잘못된 생각이다. 약어가 마침표들로 구분되는 경우가 있기 때문이다.   

#### 예제 11: 문장 경계 인식
###### 이 예제에서는 서로 다른 두 문장에 있는 단어 'bank'의 의미를 찾아본다. 

1. 주피터 노트북(jupyter notebook)을 연다.
2. 새 셀을 삽입하고 다음 코드를 추가해 필요한 라이브러리를 불러온다.
![이미지]({{ site.url }}/images/예제11-1.png)
3. sent_tokenize() 함수를 이용해 주어진 텍스트에서 문장을 인식한다.
![이미지]({{ site.url }}/images/예제11-2.png)
그림에서 볼 수 있듯이 주어진 텍스트에서 문장이 분리되어 List 형태로 바뀌는 것을 볼 수 있다.  

###### 나의 경우 비정형 데이터를 정형으로 바꾸는 작업을 한 번 해본적이 있는데  문서의 본문을 추출 해서 본문에서 중요한 NER을 추출하는 작업이 였다. 그때 NER을 추출하기 위해 했던 전처리 작업으로 위의 나열 된 항목 중 문장 경계 인식, PoS 태깅, 불용어 처리, Ner 등을 이용했고 추가로 동의어와 시맨틱 패턴 기능을 이용했었다.  
###### 프로세스는 문서 원문 추출 -> 문단 분리 -> 문장 분리 -> PoS 태깅 -> 시맨틱 패턴 매칭 -> Ner 추출 형태로 작업이 진행 됬었다. 마지막에 Ner 추출 단계에서 추출 된 Ner들을 데이터베이스에 넣는 것이였다. 그 작업을 하면서 비정형 데이터를 핸들링 한다는게 결코 쉬운일이 아니란것을 많ㅇ
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTcyMDk4MDc4OSwxODE0MjY3NTQ1LC0xMz
YyNTExNDUxLC0xODg1NDIwNTc5LDk5MjAxNjYyNiwtMTI3MDQ5
MDk5MCwyMDA4NzgwNjY1LC0xNDkxNTU5MjYyLDE1ODc3ODUxOT
ksLTE3NDI2MjQzMzcsLTE3Nzc0NzMwMTYsODEzNjk4NDA0LC0x
Mjk2NDE0Mzc0LC0xNzg1MjUzNDI1LC0xNjM1MTg1MDU0LDIwOD
k5NTQ2MzAsMjg3NzM2MjUxLC02MTM0NzI4MjBdfQ==
-->